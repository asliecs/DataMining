---
title: "Tarea 7"
author: "Aslie Cárdenas Sandoval"
date: "2024-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(traineR)
library(caret)
```

<h2 style="color:#FF1493;">Ejercicio 1</h2>
Esta pregunta utiliza los datos (tumores.csv). Se trata de un conjunto de datos de características del tumor cerebral que incluye cinco variables de primer orden y ocho de textura y cuatro parámetros de evaluación de la calidad con el nivel objetivo. Las variables son: Media, Varianza, Desviación estándar, Asimetría, Kurtosis, Contraste, Energía, ASM (segundo momento angular), Entropía, Homogeneidad, Disimilitud, Correlación, Grosor, PSNR (Pico de la relación señal-ruido), SSIM (Índice de Similitud Estructurada), MSE (Mean Square Error), DC (Coeficiente de Dados) y la variable a predecir tipo (1 = Tumor, 0 = No-Tumor).

<h3 style="color:steelblue;">Ejercicio 1.1</h4>
Cargue la tabla de datos tumores.csv en R y genere en R usando la función createDataPartition(...) del paquete caret la tabla de testing con una 25 % de los datos y con el resto de los datos genere una tabla de aprendizaje.
```{r}
datos.tumores <- read.csv("./DatosTarea7/tumores.csv", header = TRUE, sep = ",", dec = '.', stringsAsFactors = TRUE, row.names = 1)
datos.tumores$tipo <- as.factor(datos.tumores$tipo)
dim(datos.tumores)

set.seed(123) 
muestra <- createDataPartition(y = datos.tumores$tipo, p = 0.75, list = FALSE)

taprendizaje <- datos.tumores[muestra, ]
ttesting <- datos.tumores[-muestra, ]
```


<br>
<h3 style="color:steelblue;">Ejercicio 1.2</h4>
Usando Máquinas de Soporte Vectorial, con todos los núcleos (kernel) (en traineR) genere un modelos predictivos para la tabla de aprendizaje.
```{r}
modelo.sigmoid <- train.svm(tipo~.,data = taprendizaje, kernel = "sigmoid")
prediccion.sigmoid <- predict(modelo.sigmoid, ttesting , type = "class")
mc.sigmoid <- confusion.matrix(ttesting, prediccion.sigmoid)
cat("Índices generales usando kernel sigmoid:\n")
indices.sigmoid <- general.indexes(mc = mc.sigmoid)
indices.sigmoid

```
```{r}
modelo.linear <- train.svm(tipo~.,data = taprendizaje, kernel = "linear")
prediccion.linear <- predict(modelo.linear, ttesting , type = "class")
mc.linear <- confusion.matrix(ttesting, prediccion.linear)
cat("Índices generales usando kernel linear:\n")
indices.linear <- general.indexes(mc = mc.linear)
indices.linear
```
```{r}
modelo.polynomial <- train.svm(tipo~.,data = taprendizaje, kernel = "polynomial")
prediccion.polynomial <- predict(modelo.polynomial, ttesting , type = "class")
mc.polynomial <- confusion.matrix(ttesting, prediccion.polynomial)
cat("Índices generales usando kernel polynomial:\n")
indices.polynomial <- general.indexes(mc = mc.polynomial)
indices.polynomial
```

```{r}
modelo.radial <- train.svm(tipo~.,data = taprendizaje, kernel = "radial")
prediccion.radial <- predict(modelo.radial, ttesting , type = "class")
mc.radial <- confusion.matrix(ttesting, prediccion.radial)
cat("Índices generales usando kernel radial:\n")
indices.radial <- general.indexes(mc = mc.radial)
indices.radial
```

<br>
<h3 style="color:steelblue;">Ejercicio 1.3</h4>
Construya una tabla para los índices anteriores que permita comparar el resultado de Máquinas de Soporte Vectorial con respecto a los métodos generados en las tareas anteriores ¿Cuál método es mejor?
```{r}
library(traineR)
library(dplyr)
nucleos <- c("rectangular", "triangular", "epanechnikov", "biweight", 
             "triweight", "cos", "inv", "gaussian", "optimal")


resultados.knn <- data.frame(
  Método = character(),
  Precision = numeric(),
  stringsAsFactors = FALSE
)


for (nucleo in nucleos) {
  modelo.knn <- train.knn(tipo ~ ., data = taprendizaje, kernel = nucleo)
  prediccion.knn <- predict(modelo.knn, ttesting, type = "class")
  mc.knn <- confusion.matrix(ttesting, prediccion.knn)
  indices.knn <- general.indexes(mc = mc.knn)
  resultados.knn <- rbind(
    resultados.knn,
    data.frame(
      Método = paste("KNN (", nucleo, ")", sep = ""),
      Precision = indices.knn$overall.accuracy
    )
  )
}



resultados <- rbind(
  data.frame(
    Método = c("SVM (Sigmoid)", "SVM (Linear)", "SVM (Polynomial)", "SVM (Radial)"),
    Precision = c(indices.sigmoid$overall.accuracy, indices.linear$overall.accuracy,
                  indices.polynomial$overall.accuracy, indices.radial$overall.accuracy)
  ),
  resultados.knn
)

resultados.ordenados <- resultados %>%
  arrange(desc(Precision))

resultados.ordenados

```
Comparando el SVM con el KNN el mejor método es el linear de SVM, es el que tiene mayor presición.


<h2 style="color:#FF1493;">Ejercicio 2</h2>
Esta pregunta utiliza los datos sobre la conocida historia y tragedia del Titanic, usando los datos titanicV2020.csv de los pasajeros se trata de predecir la supervivencia o no de un pasajero.
La tabla contiene 12 variables y 1309 observaciones, las variables son:

- PassegerId: El código de identificación del pasajero (valor único).
- Survived: Variable a predecir, 1 (el pasajero sobrevivió) 0 (el pasajero no sobrevivió).
- Pclass: En que clase viajaba el pasajero (1 = primera, 2 = segunda , 3 = tercera).
- Name: Nombre del pasajero (valor único).
- Sex: Sexo del pasajero.
- Age: Edad del pasajero.
- SibSp: Cantidad de hermanos o cónyuges a bordo del Titanic.
- Parch: Cantidad de padres o hijos a bordo del Titanic.
- Ticket: Número de tiquete (valor único).
- Fare: Tarifa del pasajero.
- Cabin: Número de cabina (valor único).
- Embarked: Puerto donde embarco el pasajero (C = Cherbourg, Q = Queenstown, S = Southampton).

<h3 style="color:steelblue;">Ejercicio 2.1</h4>
Cargue la tabla de datos titanicV2020.csv, asegúrese re-codificar las variables cualitativas y de ignorar variables que no se deben usar.
```{r}
library(caret)
library(traineR)

titanic <- read.csv("./DatosTarea7/titanicV2020.csv", header = TRUE, sep = ",", dec = '.', stringsAsFactors = TRUE, row.names = 1)
titanic$Survived <- factor(titanic$Survived)
titanic$Pclass <- factor(titanic$Pclass, ordered = TRUE)

titanic <- titanic[, !names(titanic) %in% c("Name", "Ticket", "Cabin")]
titanic <- na.omit(titanic)
```


<br>
<h3 style="color:steelblue;">Ejercicio 2.2</h4>
Usando el comando sample de R genere al azar una tabla aprendizaje con un 80 % de los datos y con el resto de los datos genere una tabla de aprendizaje.
```{r}
set.seed(123)
n <- nrow(titanic)
muestra <- sample(1:n, size = floor(n * 0.80))
aprendizaje.titanic <- titanic[muestra, ]
testing.titanic <- titanic[-muestra, ]

testing.titanic$Sex <- factor(testing.titanic$Sex, levels = levels(aprendizaje.titanic$Sex))
testing.titanic$Embarked <- factor(testing.titanic$Embarked, levels = levels(aprendizaje.titanic$Embarked))

cat("Número de observaciones en el conjunto de entrenamiento:", nrow(aprendizaje.titanic), "\n")
cat("Número de observaciones en el conjunto de prueba:", nrow(testing.titanic), "\n")


```

<br>
<h3 style="color:steelblue;">Ejercicio 2.3</h4>
Genere un Modelo Predictivo usando SVM, con el paquete traineR, luego para este modelo calcule la matriz de confusión, la precisión, la precisión positiva, la precisión negativa, los falsos positivos, los falsos negativos, la acertividad positiva y la acertividad negativa. Utilice el Kernel que dé mejores resultados.
```{r}

indices.confusion2 <- function(MC) {
  if (nrow(MC) != ncol(MC)) {
    stop("La matriz de confusión debe ser cuadrada.")
  }
  
  precision.global <- sum(diag(MC)) / sum(MC)
  error.global <- 1 - precision.global
  precision.categoria <- diag(MC) / rowSums(MC)
  
  VP <- MC[2,2]  # Verdaderos Positivos
  FP <- MC[1,2]  # Falsos Positivos
  FN <- MC[2,1]  # Falsos Negativos
  VN <- MC[1,1]  # Verdaderos Negativos
  
  precision.positiva <- VP / (VP + FN)
  precision.negativa <- VN / (VN + FP)
  proporcion.falsos.positivos <- FP / (VN + FP)
  proporcion.falsos.negativos <- FN / (VP + FN)
  asertividad.positiva <- VP / (VP + FP)
  asertividad.negativa <- VN / (VN + FN)
  
  res <- list(
    matriz.confusion = MC,
    precision.global = precision.global,
    error.global = error.global,
    precision.categoria = precision.categoria,
    precision.positiva = precision.positiva,
    precision.negativa = precision.negativa,
    falsos.positivos = proporcion.falsos.positivos,
    falsos.negativos = proporcion.falsos.negativos,
    asertividad.positiva = asertividad.positiva,
    asertividad.negativa = asertividad.negativa
  )
  
  names(res) <- c(
    "Matriz de Confusión", "Precisión Global", "Error Global",
    "Precisión por categoría", "Precisión Positiva", "Precisión Negativa",
    "Falsos Positivos", "Falsos Negativos", "Asertividad Positiva", "Asertividad Negativa"
  )
  
  return(res)
}

# Función para entrenar SVM con diferentes kernels y evaluar
evaluate.svm <- function(kernel) {
  modelo <- train.svm(Survived ~ ., data = aprendizaje.titanic, kernel = kernel)
  prediccion <- predict(modelo, newdata = testing.titanic)
  mc <- table(Actual = testing.titanic$Survived, Predicted = prediccion$prediction)
  indices <- indices.confusion2(mc)
  return(list(kernel = kernel, indices = indices, accuracy = indices$`Precisión Global`))
}

# Probar diferentes kernels
kernels <- c("linear", "polynomial", "radial", "sigmoid")
resultados <- lapply(kernels, evaluate.svm)


# Encontrar y mostrar el mejor kernel basado en precisión global
mejor_kernel <- kernels[which.max(sapply(resultados, function(x) x$accuracy))]
cat("El mejor kernel basado en Precisión Global es:", mejor_kernel, "\n")

```


<br>
<h3 style="color:steelblue;">Ejercicio 2.4</h4>
Construya una tabla para los índices anteriores que permita comparar el resultado de los métodos SVM con respecto a los métodos de las tareas anteriores ¿Cuál método es mejor?
```{r}
library(knitr)

evaluate.knn <- function(kernel) {
  modelo <- train.knn(Survived ~ ., data = aprendizaje.titanic, kernel = kernel)
  prediccion <- predict(modelo, newdata = testing.titanic)
  mc <- table(Actual = testing.titanic$Survived, Predicted = prediccion$prediction)
  indices <- indices.confusion2(mc)
  return(list(kernel = kernel, indices = indices, accuracy = indices$`Precisión Global`))
}

nucleos <- c("rectangular", "triangular", "epanechnikov", "biweight", 
             "triweight", "cos", "inv", "gaussian", "optimal")

resultados2 <- lapply(nucleos, evaluate.knn)


comparacion <- data.frame(
  Kernel = sapply(resultados, function(x) x$kernel),
  `Precisión Global` = sapply(resultados, function(x) x$indices$`Precisión Global`),
  `Precisión Positiva` = sapply(resultados, function(x) x$indices$`Precisión Positiva`),
  `Precisión Negativa` = sapply(resultados, function(x) x$indices$`Precisión Negativa`),
  `Asertividad Positiva` = sapply(resultados, function(x) x$indices$`Asertividad Positiva`),
  `Asertividad Negativa` = sapply(resultados, function(x) x$indices$`Asertividad Negativa`)
)


tabla.comparativa.svm <- kable(comparacion, 
                               format = "markdown",
                               align = c('l', rep('r', 5)),
                               col.names = c("Kernel", "PG", "Precisión Pos", 
                                             "Precisión Neg", "Asertividad Pos", 
                                             "Asertividad Neg"),
                               caption = "Comparación de Kernels SVM")


comparacion2 <- data.frame(
  Kernel = sapply(resultados2, function(x) x$kernel),
  `Precisión Global` = sapply(resultados2, function(x) x$indices$`Precisión Global`),
  `Precisión Positiva` = sapply(resultados2, function(x) x$indices$`Precisión Positiva`),
  `Precisión Negativa` = sapply(resultados2, function(x) x$indices$`Precisión Negativa`),
  `Asertividad Positiva` = sapply(resultados2, function(x) x$indices$`Asertividad Positiva`),
  `Asertividad Negativa` = sapply(resultados2, function(x) x$indices$`Asertividad Negativa`)
)


tabla.comparativa.knn <- kable(comparacion2, 
                               format = "markdown",
                               align = c('l', rep('r', 5)),
                               col.names = c("Kernel", "PG", "Precisión Pos", 
                                             "Precisión Neg", "Asertividad Pos", 
                                             "Asertividad Neg"),
                               caption = "Comparación de Kernels KNN")



comparacion.total <- rbind(
  cbind(Método = "SVM", comparacion),
  cbind(Método = "KNN", comparacion2)
)


tabla.comparativa.total <- kable(comparacion.total, 
                                 format = "markdown",
                                 align = c('l', 'l', rep('r', 5)),
                                 col.names = c("Método", "Kernel", "PG", "Precisión Pos", 
                                               "Precisión Neg", "Asertividad Pos", 
                                               "Asertividad Neg"),
                                 caption = "Comparación de SVM y KNN")


tabla.comparativa.svm

tabla.comparativa.knn

tabla.comparativa.total
```

Comparando el KNN con el SVM, el KNN tiene la presición global más alta con los núcleos epanechnikov, cos y gaussian. Y con el SVM el mejor es el radial.


<h2 style="color:#FF1493;">Ejercicio 3</h2>
En este ejercicio vamos a predecir números escritos a mano (Hand Written Digit Recognition), la tabla de de datos está en el archivo ZipData 2020.csv.

<h3 style="color:steelblue;">Ejercicio 3.1</h4>
Cargue la tabla de datos ZipData 2020.csv en R.
```{r}
datos.numeros <- read.csv("./DatosTarea7/ZipData_2020.csv", header = TRUE, sep = ";", dec = '.', stringsAsFactors = TRUE)
dim(datos.numeros)

```


<br>
<h3 style="color:steelblue;">Ejercicio 3.2</h4>
Use el método de Máquinas de Soporte Vectorial con el núcleo y los parámetros que usted considere más conveniente para generar un modelo predictivo para la tabla ZipData 2020.csv usando el 80 % de los datos para la tabla aprendizaje y un 20 % para la tabla testing, luego
calcule para los datos de testing la matriz de confusión, la precisión global y la precisión para cada una de las categorías. ¿Son buenos los resultados? Explique.
```{r}
set.seed(123)  
muestra.numeros <- createDataPartition(y = datos.numeros$Numero, p = 0.8, list = F)
taprendizaje.num <- datos.numeros[muestra.numeros, ]
ttesting.num <- datos.numeros[-muestra.numeros, ]

model <- train.svm(Numero ~ ., data = taprendizaje.num)

prediccion <- predict(model, ttesting.num)


mc <- confusion.matrix(ttesting.num, prediccion)

indices <- general.indexes(mc = mc)
indices

```

- Los resultados son buenos la precisión general es muy alta 96.76%, lo que indica un alto rendimiento del modelo, la tasa de error: 3.24%, es muy baja, lo que sugiere pocas predicciones incorrectas y la precisión por categoría es generalmente alta, con la menor de 93.7% para cinco, que aún es muy aceptable.

<br>
<h3 style="color:steelblue;">Ejercicio 3.3</h4>
Compare los resultados con los obtenidos en las tareas anteriores

```{r}
n <- nrow(datos.numeros)
tamano.prueba <- floor(n * 0.80)

set.seed(123) 
muestra.numeros <- sample(1:n, tamano.prueba)

ttesting <- datos.numeros[-muestra.numeros, ]
taprendizaje <- datos.numeros[muestra.numeros, ]


modelo.numeros <- train.knn(Numero~., data = taprendizaje, kmax = floor(sqrt(n)))
prediccion <- predict(modelo.numeros, ttesting)

MC<- confusion.matrix(ttesting, prediccion)
indice <- general.indexes(mc = MC)
indice

```
El método SVM presenta mejores resultados en cuánto a precisión global y la precisión para cada una de las categorías, sin embargo la diferencia es mínima.

<h2 style="color:#FF1493;">Ejercicio 4</h2>
Suponga que se tiene la siguiente tabla de datos:
```{r cache=TRUE}
datos.hiper <- data.frame(
  X = c(1, 1, 1, 3, 1, 3, 1, 3, 1),
  Y = c(0, 0, 1, 1, 1, 2, 2, 2, 1),
  Z = c(1, 2, 2, 4, 3, 3, 1, 1, 0),
  Clase = factor(c("Rojo", "Rojo", "Rojo", "Rojo", "Rojo", "Azul", "Azul", "Azul", "Azul"))
)

datos.hiper

```


<h3 style="color:steelblue;">Ejercicio 4.1</h4>
Dibuje con colores los puntos de ambas clases en R3.
```{r}
library(caret)
library(traineR)
library(ggplot2)
library(lattice)
library(scatterplot3d)


colores <- ifelse(datos.hiper$Clase == "Rojo", "red", "blue")

dibujo <- scatterplot3d(datos.hiper$X, datos.hiper$Y, datos.hiper$Z, pch = 16, 
                     color = colores,,
                     main = "Clasificación de puntos en R3",
                     xlab = "X", ylab = "Y", zlab = "Z")
```


<br>
<h3 style="color:steelblue;">Ejercicio 4.2</h4>
Dibuje el hiperplano óptimo de separación e indique la ecuación de dicho hiperplano de la forma ax + by + cz + d = 0. Nota: Se debe observar con detenimiento los puntos de ambas clases para encontrar los vectores de soporte de cada margen y trazar con estos puntos los hiperplanos de los márgenes luego trazar el hiperplano de soporte justo en el centro.
```{r}
library(e1071)
# Ajustar el modelo SVM con kernel lineal
svmfit <- svm(Clase ~ X + Y + Z, data = datos.hiper, kernel = "linear", cost = 10, scale = FALSE)

# Imprimir el modelo ajustado
print(svmfit)

# Coeficientes del hiperplano
beta <- drop(t(svmfit$coefs) %*% as.matrix(datos.hiper[, c("X", "Y", "Z")])[svmfit$index, ])
beta0 <- svmfit$rho

# Mostrar la ecuación del hiperplano
cat(sprintf("Ecuación del hiperplano: %.2f * X + %.2f * Y + %.2f * Z + %.2f = 0\n", 
            beta[1], beta[2], beta[3], beta0))

# Graficar en 3D con los puntos de datos, vectores de soporte y el hiperplano
colores <- ifelse(datos.hiper$Clase == "Rojo", "red", "blue")
dibujo <- scatterplot3d(datos.hiper$X, datos.hiper$Y, datos.hiper$Z, pch = 19, 
                        color = colores, main = "Clasificación de puntos en R3",
                        xlab = "X", ylab = "Y", zlab = "Z")



```



